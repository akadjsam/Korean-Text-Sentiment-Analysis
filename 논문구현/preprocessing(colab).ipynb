{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyMUxmiKc8j88QmqpJUwOO6t"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"5Kcn81QuHrDc"},"outputs":[],"source":["# 전처리 결과 시각화\n","\n","import pandas as pd\n","import re\n","from transformers import BertTokenizer\n","from collections import Counter\n","\n","def clean_korean_text(text):\n","    \"\"\"\n","    한글, 공백 외 문자 제거 및 연속 공백을 단일 공백으로 변환\n","    \"\"\"\n","    cleaned = re.sub(r'[^가-힣\\s]', '', str(text)) # 가~힣, 공백을 제외한 나머지 제거\n","    cleaned = re.sub(r'\\s+', ' ', cleaned)\n","    return cleaned.strip()\n","\n","def remove_stopwords(text, stopwords):\n","    \"\"\"\n","    불용어 제거\n","    \"\"\"\n","    words = text.split()\n","    filtered_words = [word for word in words if word not in stopwords]\n","    return ' '.join(filtered_words)\n","\n","def rating_to_sentiment(rating):\n","    \"\"\"\n","    평점을 감성 라벨(0~3, 4가지)로 변환. 3점은 None으로 처리.\n","    \"\"\"\n","    try:\n","        rating = int(rating)\n","        if rating == 5: return 3   # 매우 긍정적\n","        elif rating == 4: return 2 # 긍정적\n","        elif rating == 2: return 1 # 부정적\n","        elif rating == 1: return 0 # 매우 부정적\n","        else: return None\n","    except:\n","        return None\n","\n","def tokenize_with_bert(text, tokenizer, max_length=128):\n","    \"\"\"\n","    BERT 토크나이저로 텍스트를 토큰화\n","    \"\"\"\n","    encoded = tokenizer.encode_plus(\n","        text,\n","        add_special_tokens=True,\n","        max_length=max_length,\n","        padding='max_length',\n","        truncation=True,\n","        return_tensors='pt'\n","    )\n","    # 토큰 ID를 실제 토큰으로 변환\n","    input_ids = encoded['input_ids'].squeeze().tolist()\n","    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n","\n","    # 패딩 토큰이 아닌 실제 토큰만 필터링\n","    actual_tokens = []\n","    actual_token_ids = []\n","    for token_id, token in zip(input_ids, tokens):\n","        if token_id != tokenizer.pad_token_id:\n","            actual_tokens.append(token)\n","            actual_token_ids.append(token_id)\n","\n","    return {\n","        'input_ids': input_ids,\n","        'attention_mask': encoded['attention_mask'].squeeze().tolist(),\n","        'tokens': tokens,  # 패딩 포함 전체 토큰\n","        'actual_tokens': actual_tokens,  # 패딩 제외 실제 토큰\n","        'actual_token_ids': actual_token_ids  # 패딩 제외 실제 토큰 ID\n","    }\n","\n","def save_processed_data(df, output_path):\n","    df.to_csv(output_path, index=False, encoding='utf-8')\n","    print(f\"전처리된 데이터 저장 완료 : {output_path}\")\n","\n","def preprocess_naver_shopping_data(file_path):\n","    \"\"\"\n","    네이버 쇼핑 리뷰 데이터 전처리\n","    \"\"\"\n","\n","    # 데이터 로드\n","    try:\n","        df = pd.read_csv(file_path, sep='\\t', header=None, names=['rating', 'review'])\n","        print(f\"총 {len(df)}개의 리뷰 로드 완료\")\n","    except Exception as e:\n","        print(f\"파일 로딩 오류: {e}\")\n","        return None, None\n","\n","    # 텍스트 정제\n","    print(\"텍스트 정제\")\n","    df['cleaned_review'] = df['review'].apply(clean_korean_text)\n","    df = df[df['cleaned_review'].str.len() > 0] # 빈 리뷰 제거\n","\n","    # 중복 리뷰 제거\n","    print(\"중복 리뷰 제거\")\n","    before_dedup = len(df)\n","    df = df.drop_duplicates(subset=['cleaned_review'], keep='first')\n","    print(f\"{before_dedup - len(df)}개 중복 리뷰 제거 완료, {len(df)}개 남음\")\n","\n","    # 불용어 제거\n","    print(\"불용어 제거\")\n","    stopwords = ['은', '는', '이', '가', '고', '을', '를']\n","    df['processed_review'] = df['cleaned_review'].apply(lambda x: remove_stopwords(x, stopwords))\n","\n","    # 감성 라벨 매핑\n","    print(\"감성 라벨 매핑\")\n","    df['sentiment_label'] = df['rating'].apply(rating_to_sentiment)\n","    df = df.dropna(subset=['sentiment_label']) # 3점 리뷰 등 None 값 제거\n","    df['sentiment_label'] = df['sentiment_label'].astype(int)\n","\n","    # 데이터 분포 확인\n","    sentiment_counts = df['sentiment_label'].value_counts().sort_index()\n","    sentiment_names = {0: '매우 부정적', 1: '부정적', 2: '긍정적', 3: '매우 긍정적'}\n","    print(\"\\n감성 라벨 분포:\")\n","    #print(sentiment_counts)\n","    for label, count in sentiment_counts.items():\n","        print(f\"  {sentiment_names[label]} ({label}): {count}개\")\n","\n","    return df\n","\n","if __name__ == \"__main__\":\n","    FILE_PATH = 'naver_shopping.txt'\n","\n","    # 데이터 전처리\n","    processed_df = preprocess_naver_shopping_data(FILE_PATH)\n","    save_processed_data(processed_df, 'preprocessed_reviews.csv') # csv 형식으로 저장\n","\n","    if processed_df is not None:\n","        # 전처리된 데이터프레임 확인\n","        print(\"\\n전처리 완료된 데이터프레임 (상위 5개)\")\n","        print(processed_df.head())\n","\n","        # 샘플 토큰화 진행 및 결과 확인\n","        print(\"\\n샘플 토큰화 결과\")\n","        tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n","\n","        for index, row in processed_df.head(3).iterrows():\n","            review_text = row['processed_review']\n","            tokens_result = tokenize_with_bert(review_text, tokenizer)\n","\n","            #actual_token_count = len([token_id for token_id in tokens['input_ids'] if token_id != tokenizer.pad_token_id])\n","\n","            # print(f\"\\n샘플 {index+1}\")\n","            # print(f\"전처리된 리뷰: {review_text}\")\n","            # print(f\"토큰 ID (일부): {tokens['input_ids'][:15]}...\")\n","            # print(f\"토큰화된 길이(패딩 포함): {len(tokens['input_ids'])}\")\n","            # print(f\"실제 토큰 길이: {actual_token_count}\") # 패딩 제외\n","            print(f\"\\n{'='*50}\")\n","            print(f\"샘플 {index+1}\")\n","            print(f\"{'='*50}\")\n","            print(f\"원본 리뷰: {row['review']}\")\n","            print(f\"전처리된 리뷰: {review_text}\")\n","            print(f\"\\n실제 토큰들 (패딩 제외): {tokens_result['actual_tokens']}\")\n","            print(f\"실제 토큰 개수: {len(tokens_result['actual_tokens'])}\")\n","            print(f\"\\n토큰 ID (실제): {tokens_result['actual_token_ids']}\")\n","            print(f\"토큰 ID (패딩 포함, 일부): {tokens_result['input_ids'][:15]}...\")\n","            print(f\"전체 길이 (패딩 포함): {len(tokens_result['input_ids'])}\")\n","\n","            # 토큰별 ID\n","            print(f\"\\n토큰별 상세 분석:\")\n","            for i, (token, token_id) in enumerate(zip(tokens_result['actual_tokens'], tokens_result['actual_token_ids'])):\n","                print(f\"  {i+1:2d}: '{token}' (ID: {token_id})\")\n","                if i >= 10:  # 처음 10개만 출력\n","                    print(f\"  ... (총 {len(tokens_result['actual_tokens'])}개 토큰)\")\n","                    break"]}]}