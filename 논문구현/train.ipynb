{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"## ëª¨ë¸ ì´ì–´ì„œ í•™ìŠµí•  ë•Œ\nimport pandas as pd\nimport re\nimport os\nimport torch\nimport json\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import BertTokenizer, BertForSequenceClassification, get_linear_schedule_with_warmup\nfrom torch.optim import AdamW\n#from tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nimport numpy as np\nimport math\nfrom tqdm.notebook import tqdm\n\ndef clean_korean_text(text):\n    cleaned = re.sub(r'[^ê°€-í£\\s]', '', str(text))\n    cleaned = re.sub(r'\\s+', ' ', cleaned)\n    return cleaned.strip()\n\ndef remove_stopwords(text, stopwords):\n    words = text.split()\n    filtered_words = [word for word in words if word not in stopwords]\n    return ' '.join(filtered_words)\n\ndef rating_to_sentiment(rating):\n    try:\n        rating = int(rating)\n        if rating == 5: return 3\n        elif rating == 4: return 2\n        elif rating == 2: return 1\n        elif rating == 1: return 0\n        else: return None\n    except:\n        return None\n\ndef preprocess_naver_shopping_data(file_path):\n    try:\n        df = pd.read_csv(file_path, sep='\\t', header=None, names=['rating', 'review'])\n        print(f\"ì´ {len(df)}ê°œì˜ ë¦¬ë·° ë¡œë“œ ì™„ë£Œ\")\n    except Exception as e:\n        print(f\"íŒŒì¼ ë¡œë”© ì˜¤ë¥˜: {e}\")\n        return None\n\n    print(\"í…ìŠ¤íŠ¸ ì •ì œ ì¤‘...\")\n    df['cleaned_review'] = df['review'].apply(clean_korean_text)\n    df = df[df['cleaned_review'].str.len() > 0]\n\n    print(\"ì¤‘ë³µ ë¦¬ë·° ì œê±° ì¤‘...\")\n    before_dedup = len(df)\n    df = df.drop_duplicates(subset=['cleaned_review'], keep='first')\n    print(f\"{before_dedup - len(df)}ê°œ ì¤‘ë³µ ë¦¬ë·° ì œê±° ì™„ë£Œ, {len(df)}ê°œ ë‚¨ìŒ\")\n\n    print(\"ë¶ˆìš©ì–´ ì œê±° ì¤‘...\")\n    stopwords = ['ì€', 'ëŠ”', 'ì´', 'ê°€', 'ê³ ', 'ì„', 'ë¥¼']\n    df['processed_review'] = df['cleaned_review'].apply(lambda x: remove_stopwords(x, stopwords))\n\n    print(\"ê°ì„± ë¼ë²¨ ë§¤í•‘ ì¤‘...\")\n    df['sentiment_label'] = df['rating'].apply(rating_to_sentiment)\n    df = df.dropna(subset=['sentiment_label'])\n    df['sentiment_label'] = df['sentiment_label'].astype(int)\n\n    sentiment_counts = df['sentiment_label'].value_counts().sort_index()\n    sentiment_names = {0: 'ë§¤ìš° ë¶€ì •ì ', 1: 'ë¶€ì •ì ', 2: 'ê¸ì •ì ', 3: 'ë§¤ìš° ê¸ì •ì '}\n    print(\"\\nê°ì„± ë¼ë²¨ ë¶„í¬:\")\n    for label, count in sentiment_counts.items():\n        print(f\"  {sentiment_names[label]} ({label}): {count}ê°œ\")\n    return df\n\nclass ReviewDataset(Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        item['labels'] = torch.tensor(self.labels[idx])\n        return item\n\n    def __len__(self):\n        return len(self.labels)\n\ndef evaluate_model(model, data_loader, device):\n    \"\"\"ëª¨ë¸ í‰ê°€ í•¨ìˆ˜\"\"\"\n    model.eval()\n    predictions = []\n    true_labels = []\n    total_loss = 0\n    \n    with torch.no_grad():\n        for batch in tqdm(data_loader, desc=\"Evaluating\"):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['labels'].to(device)\n            \n            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n            loss = outputs.loss\n            total_loss += loss.item()\n            \n            preds = torch.argmax(outputs.logits, dim=-1)\n            predictions.extend(preds.cpu().numpy())\n            true_labels.extend(labels.cpu().numpy())\n    \n    avg_loss = total_loss / len(data_loader)\n    accuracy = accuracy_score(true_labels, predictions)\n    \n    return avg_loss, accuracy, true_labels, predictions\n\ndef save_checkpoint(model, tokenizer, optimizer, scheduler, epoch, best_val_acc, patience_counter, checkpoint_dir):\n    \"\"\"ì²´í¬í¬ì¸íŠ¸ ì €ì¥ í•¨ìˆ˜ (ìŠ¤ì¼€ì¤„ëŸ¬ í¬í•¨)\"\"\"\n    if not os.path.exists(checkpoint_dir):\n        os.makedirs(checkpoint_dir)\n    \n    # ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ì €ì¥\n    model.save_pretrained(checkpoint_dir)\n    tokenizer.save_pretrained(checkpoint_dir)\n    \n    # í•™ìŠµ ìƒíƒœë¥¼ torch.saveë¡œ ì €ì¥ (ìŠ¤ì¼€ì¤„ëŸ¬ í¬í•¨)\n    checkpoint_data = {\n        'epoch': epoch,\n        'optimizer_state_dict': optimizer.state_dict(),\n        'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n        'best_val_acc': best_val_acc,\n        'patience_counter': patience_counter\n    }\n    \n    torch.save(checkpoint_data, os.path.join(checkpoint_dir, 'training_state.pth'))\n    print(f\"ì²´í¬í¬ì¸íŠ¸ê°€ '{checkpoint_dir}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n\ndef load_checkpoint(checkpoint_dir, device, total_steps=None):\n    \"\"\"ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ í•¨ìˆ˜ (ìŠ¤ì¼€ì¤„ëŸ¬ í¬í•¨)\"\"\"\n    try:\n        # ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ\n        model = BertForSequenceClassification.from_pretrained(checkpoint_dir)\n        tokenizer = BertTokenizer.from_pretrained(checkpoint_dir)\n        \n        # í•™ìŠµ ìƒíƒœë¥¼ torch.loadë¡œ ë¡œë“œ\n        checkpoint_data = torch.load(os.path.join(checkpoint_dir, 'training_state.pth'), map_location=device, weights_only=False)\n\n        \n        model.to(device)\n        optimizer = AdamW(model.parameters(), lr=2e-5)  # ê³„ì† í•™ìŠµì‹œ ë‚®ì€ learning rate\n        optimizer.load_state_dict(checkpoint_data['optimizer_state_dict'])\n        \n        # ìŠ¤ì¼€ì¤„ëŸ¬ ë¡œë“œ\n        scheduler = None\n        if checkpoint_data.get('scheduler_state_dict') and total_steps:\n            scheduler = get_linear_schedule_with_warmup(\n                optimizer, num_warmup_steps=int(0.1 * total_steps), num_training_steps=total_steps\n            )\n            scheduler.load_state_dict(checkpoint_data['scheduler_state_dict'])\n        \n        # optimizer state ë‚´ í…ì„œë“¤ì„ í˜„ì¬ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™\n        for state in optimizer.state.values():\n            for k, v in state.items():\n                if isinstance(v, torch.Tensor):\n                    state[k] = v.to(device)\n        \n        print(f\"ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì™„ë£Œ: ì—í­ {checkpoint_data['epoch']}ë¶€í„° ì¬ê°œ\")\n        \n        return (model, tokenizer, optimizer, scheduler,\n                checkpoint_data['epoch'], \n                checkpoint_data['best_val_acc'], \n                checkpoint_data['patience_counter'])\n    \n    except Exception as e:\n        print(f\"ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì‹¤íŒ¨: {e}\")\n        return None\n\ndef save_data_split(train_texts, train_labels, val_texts, val_labels, test_texts, test_labels, save_path):\n    \"\"\"ë°ì´í„° ë¶„í•  ê²°ê³¼ ì €ì¥ (torch.save ì‚¬ìš©)\"\"\"\n    data_split = {\n        'train_texts': train_texts,\n        'train_labels': train_labels,\n        'val_texts': val_texts,\n        'val_labels': val_labels,\n        'test_texts': test_texts,\n        'test_labels': test_labels\n    }\n    \n    torch.save(data_split, save_path)\n    print(f\"ë°ì´í„° ë¶„í•  ê²°ê³¼ê°€ '{save_path}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n\ndef load_data_split(save_path):\n    \"\"\"ì €ì¥ëœ ë°ì´í„° ë¶„í•  ê²°ê³¼ ë¡œë“œ (torch.load ì‚¬ìš©)\"\"\"\n    try:\n        data_split = torch.load(save_path, map_location='cpu')  # ë°ì´í„°ëŠ” CPUì— ë¡œë“œ\n        print(f\"ì €ì¥ëœ ë°ì´í„° ë¶„í• ì„ '{save_path}'ì—ì„œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.\")\n        return (data_split['train_texts'], data_split['train_labels'],\n                data_split['val_texts'], data_split['val_labels'],\n                data_split['test_texts'], data_split['test_labels'])\n    except Exception as e:\n        print(f\"ì €ì¥ëœ ë°ì´í„° ë¶„í•  ë¡œë“œ ì‹¤íŒ¨: {e}\")\n        return None\n\n# ===================================================================\n# ë©”ì¸ ì‹¤í–‰ë¶€ (ì²´í¬í¬ì¸íŠ¸ ì§€ì› ë²„ì „)\n# ===================================================================\nif __name__ == \"__main__\":\n    # --- ğŸš€ í•™ìŠµ ì„¤ì • (í•„ìš”ì— ë”°ë¼ ìˆ˜ì •í•˜ì„¸ìš”) ---\n    USE_SCHEDULER = True\n    INITIAL_LR = 5e-5\n    CONTINUE_LR = 2e-5\n    batch_size = 32\n    # ì²´í¬í¬ì¸íŠ¸ ê´€ë ¨ ì„¤ì •\n    USE_CHECKPOINT = True  # True: ì²´í¬í¬ì¸íŠ¸ ì‚¬ìš©, False: ìƒˆë¡œ ì‹œì‘\n    CHECKPOINT_DIR = '/kaggle/input/bert/pytorch/epoch10/1/latest_checkpoint'\n    DATA_SPLIT_PATH = '/kaggle/input/data-split/data_split.pth'\n    # CHECKPOINT_DIR = '/kaggle/working/latest_checkpoint'  # ì²´í¬í¬ì¸íŠ¸ ì €ì¥/ë¡œë“œ ê²½ë¡œ\n    # DATA_SPLIT_PATH = '/kaggle/working/data_split.pth'  # ë°ì´í„° ë¶„í•  ê²°ê³¼ ì €ì¥ ê²½ë¡œ\n    \n    # ê¸°ë³¸ í•™ìŠµ ì„¤ì • (ì²´í¬í¬ì¸íŠ¸ê°€ ì—†ì„ ë•Œ ì‚¬ìš©)\n    START_FROM_PRETRAINED = True  # True: ì‚¬ì „í›ˆë ¨ ëª¨ë¸, False: ê¸°ì¡´ í•™ìŠµëœ ëª¨ë¸\n    SAVED_MODEL_PATH = '/kaggle/input/epochs/20epochs'  # ê¸°ì¡´ í•™ìŠµëœ ëª¨ë¸ ê²½ë¡œ(ì‚¬ìš© x)\n    PRETRAINED_MODEL_NAME = 'bert-base-multilingual-cased'  # ì‚¬ì „í›ˆë ¨ ëª¨ë¸ëª…\n    \n    TARGET_TOTAL_EPOCHS = 30  # ìµœì¢… ëª©í‘œ ì—í­ ìˆ˜\n    EPOCHS_PER_SESSION = 20  # í•œ ì„¸ì…˜ë‹¹ í•™ìŠµí•  ì—í­ ìˆ˜ (ìºê¸€ 12ì‹œê°„ ì œí•œ ê³ ë ¤)\n    \n    PROCESSED_FILE_PATH = '/kaggle/input/reviews/preprocessed_reviews.csv'\n    \n    # --- ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ í™•ì¸ ë° ë¡œë“œ ---\n    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n    print(f\"ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}\")\n    \n    checkpoint_loaded = False\n    scheduler = None\n    \n    if USE_CHECKPOINT and os.path.exists(CHECKPOINT_DIR):\n        print(\"ğŸ”„ ì´ì „ ì²´í¬í¬ì¸íŠ¸ë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. ë¡œë“œë¥¼ ì‹œë„í•©ë‹ˆë‹¤...\")\n        \n        # ìŠ¤ì¼€ì¤„ëŸ¬ ì‚¬ìš©ì‹œ ì´ ìŠ¤í… ìˆ˜ ê³„ì‚°ì´ í•„ìš”í•˜ë¯€ë¡œ ì„ì‹œë¡œ ë°ì´í„° ë¡œë“œ\n        if USE_SCHEDULER:\n            temp_df = pd.read_csv(PROCESSED_FILE_PATH)\n            temp_df = temp_df.dropna(subset=['processed_review', 'sentiment_label'])\n            train_size = int(len(temp_df) * 0.7)  # í•™ìŠµ ë°ì´í„° ë¹„ìœ¨\n            total_steps = math.ceil(train_size / batch_size) * (TARGET_TOTAL_EPOCHS)  # batch_size=32 ê°€ì •\n        else:\n            total_steps = None\n            \n        checkpoint_result = load_checkpoint(CHECKPOINT_DIR, device, total_steps)\n        \n        if checkpoint_result is not None:\n            model, tokenizer, optimizer, scheduler, last_epoch, best_val_acc, patience_counter = checkpoint_result\n            current_epoch = last_epoch + 1  # ë‹¤ìŒ ì—í­ë¶€í„° ì‹œì‘\n            checkpoint_loaded = True\n            print(f\"âœ… ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì„±ê³µ! ì—í­ {current_epoch}ë¶€í„° ì¬ê°œí•©ë‹ˆë‹¤.\")\n            \n            # ì €ì¥ëœ ë°ì´í„° ë¶„í•  ë¡œë“œ\n            data_split_result = load_data_split(DATA_SPLIT_PATH)\n            if data_split_result is not None:\n                train_texts, train_labels, val_texts, val_labels, test_texts, test_labels = data_split_result\n                print(\"âœ… ì´ì „ ë°ì´í„° ë¶„í•  ê²°ê³¼ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\")\n            else:\n                print(\"âŒ ë°ì´í„° ë¶„í•  ë¡œë“œ ì‹¤íŒ¨. ìƒˆë¡œ ë¶„í• í•©ë‹ˆë‹¤.\")\n                checkpoint_loaded = False\n    \n    # --- ğŸ“Š ë°ì´í„° ì¤€ë¹„ (ì²´í¬í¬ì¸íŠ¸ê°€ ì—†ëŠ” ê²½ìš°) ---\n    if not checkpoint_loaded:\n        print(\"ğŸ†• ìƒˆë¡œìš´ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤...\")\n        \n        # ë°ì´í„° ë¡œë“œ\n        print(f\"'{PROCESSED_FILE_PATH}'ì—ì„œ ì „ì²˜ë¦¬ëœ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.\")\n        processed_df = pd.read_csv(PROCESSED_FILE_PATH)\n        processed_df = processed_df.dropna(subset=['processed_review', 'sentiment_label'])\n        processed_df['sentiment_label'] = processed_df['sentiment_label'].astype(int)\n        print(f\"ì „ì²´ ë°ì´í„° ê°œìˆ˜: {len(processed_df)}\")\n        \n        # ë°ì´í„° ë¶„í• \n        print(\"\\në°ì´í„°ë¥¼ í•™ìŠµ/ê²€ì¦/í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ë¡œ ë¶„í• í•©ë‹ˆë‹¤...\")\n        train_texts, temp_texts, train_labels, temp_labels = train_test_split(\n            processed_df['processed_review'].tolist(),\n            processed_df['sentiment_label'].tolist(),\n            test_size=0.3, random_state=42,\n            stratify=processed_df['sentiment_label']\n        )\n        \n        val_texts, test_texts, val_labels, test_labels = train_test_split(\n            temp_texts, temp_labels, test_size=0.5, random_state=42, stratify=temp_labels\n        )\n        \n        print(f\"í•™ìŠµ ë°ì´í„°: {len(train_texts)}ê°œ\")\n        print(f\"ê²€ì¦ ë°ì´í„°: {len(val_texts)}ê°œ\") \n        print(f\"í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_texts)}ê°œ\")\n        \n        # ë°ì´í„° ë¶„í•  ê²°ê³¼ ì €ì¥\n        save_data_split(train_texts, train_labels, val_texts, val_labels, \n                       test_texts, test_labels, '/kaggle/working/data_split.pth')\n        \n        # ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ\n        if START_FROM_PRETRAINED:\n            print(f\"ì‚¬ì „í›ˆë ¨ ëª¨ë¸ '{PRETRAINED_MODEL_NAME}'ì„ ë¡œë“œí•©ë‹ˆë‹¤.\")\n            tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)\n            model = BertForSequenceClassification.from_pretrained(\n                PRETRAINED_MODEL_NAME, num_labels=4\n            )\n            optimizer = AdamW(model.parameters(), lr=INITIAL_LR)\n        else:\n            print(f\"ê¸°ì¡´ í•™ìŠµëœ ëª¨ë¸ '{SAVED_MODEL_PATH}'ì„ ë¡œë“œí•©ë‹ˆë‹¤.\")\n            tokenizer = BertTokenizer.from_pretrained(SAVED_MODEL_PATH)\n            model = BertForSequenceClassification.from_pretrained(SAVED_MODEL_PATH)\n            optimizer = AdamW(model.parameters(), lr=CONTINUE_LR)\n        \n        model.to(device)\n        \n        # ğŸ¯ í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •\n        if USE_SCHEDULER:\n            train_size = len(train_texts)\n            total_steps = math.ceil(train_size / batch_size) * TARGET_TOTAL_EPOCHS  # batch_size=32\n            warmup_steps = int(0.1 * total_steps)  # ì „ì²´ì˜ 10%ë¥¼ warmup\n            scheduler = get_linear_schedule_with_warmup(\n                optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps\n            )\n            print(f\"ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •: ì´ {total_steps} ìŠ¤í…, ì›Œë°ì—… {warmup_steps} ìŠ¤í…\")\n        else:\n            scheduler = None\n        \n        current_epoch = 1\n        best_val_acc = 0.0\n        patience_counter = 0\n    \n    # --- ğŸ”¤ í† í°í™” ë° ë°ì´í„°ë¡œë” ì¤€ë¹„ ---\n    print(\"\\ní† í°í™” ì§„í–‰ ì¤‘...\")\n    train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=128)\n    val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=128)\n    test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=128)\n    \n    train_dataset = ReviewDataset(train_encodings, train_labels)\n    val_dataset = ReviewDataset(val_encodings, val_labels)\n    test_dataset = ReviewDataset(test_encodings, test_labels)\n    \n    batch_size = 32\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n    print(\"í† í°í™” ë° ë°ì´í„°ë¡œë” ì¤€ë¹„ ì™„ë£Œ!\")\n    \n    # --- ğŸ“ˆ í•™ìŠµ ì‹œì‘ ---\n    end_epoch = min(current_epoch + EPOCHS_PER_SESSION - 1, TARGET_TOTAL_EPOCHS)\n    print(f\"\\n=== ëª¨ë¸ í•™ìŠµ ì‹œì‘ (ì—í­ {current_epoch}~{end_epoch}) ===\")\n    print(f\"ìµœì¢… ëª©í‘œ: {TARGET_TOTAL_EPOCHS} ì—í­\")\n   \n    \n    patience = 5  # Early stopping patience\n    \n    for epoch_idx in range(EPOCHS_PER_SESSION):\n        if current_epoch > TARGET_TOTAL_EPOCHS:\n            print(f\"\\nğŸ‰ ëª©í‘œ ì—í­ {TARGET_TOTAL_EPOCHS}ì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤!\")\n            break\n            \n        print(f\"\\nEpoch {current_epoch}/{TARGET_TOTAL_EPOCHS}\")\n        \n        # í•™ìŠµ\n        model.train()\n        train_loss = 0\n        loop = tqdm(train_loader, desc=f\"Training Epoch {current_epoch}\", leave=False)\n        \n        for batch in loop:\n            optimizer.zero_grad()\n            \n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['labels'].to(device)\n            \n            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n            loss = outputs.loss\n            train_loss += loss.item()\n            \n            loss.backward()\n            optimizer.step()\n            \n            # ğŸ¯ ìŠ¤ì¼€ì¤„ëŸ¬ ìŠ¤í… (ì‚¬ìš©í•˜ëŠ” ê²½ìš°)\n            if scheduler:\n                scheduler.step()\n                current_lr = scheduler.get_last_lr()[0]\n                loop.set_postfix(loss=loss.item(), lr=f\"{current_lr:.2e}\")\n            else:\n                loop.set_postfix(loss=loss.item())\n        \n        avg_train_loss = train_loss / len(train_loader)\n        \n        # ê²€ì¦\n        val_loss, val_acc, _, _ = evaluate_model(model, val_loader, device)\n        \n        print(f\"í•™ìŠµ ì†ì‹¤: {avg_train_loss:.4f}\")\n        print(f\"ê²€ì¦ ì†ì‹¤: {val_loss:.4f}\")  \n        print(f\"ê²€ì¦ ì •í™•ë„: {val_acc:.4f}\")\n        \n        # Best model ì—…ë°ì´íŠ¸\n        if val_acc > best_val_acc:\n            best_val_acc = val_acc\n            patience_counter = 0\n            \n            best_model_dir = f'/kaggle/working/best_sentiment_model'\n            if not os.path.exists(best_model_dir):\n                os.makedirs(best_model_dir)\n            model.save_pretrained(best_model_dir)\n            tokenizer.save_pretrained(best_model_dir)\n            print(f\"*** ìƒˆë¡œìš´ ìµœê³  ì„±ëŠ¥! ëª¨ë¸ì„ '{best_model_dir}'ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤. ***\")\n        else:\n            patience_counter += 1\n            print(f\"ì„±ëŠ¥ ê°œì„  ì—†ìŒ. Patience: {patience_counter}/{patience}\")\n        \n        # ì²´í¬í¬ì¸íŠ¸ ì €ì¥ (ë§¤ ì—í­ë§ˆë‹¤)\n        save_checkpoint(model, tokenizer, optimizer, scheduler, current_epoch, best_val_acc, patience_counter, '/kaggle/working/latest_checkpoint')\n        \n        # Early stopping ì²´í¬\n        if patience_counter >= patience:\n            print(f\"\\nEarly stopping! {patience} ì—í­ ë™ì•ˆ ì„±ëŠ¥ ê°œì„ ì´ ì—†ì—ˆìŠµë‹ˆë‹¤.\")\n            break\n        \n        current_epoch += 1\n    \n    # --- ğŸ“Š ì„¸ì…˜ ì¢…ë£Œ ì‹œ ìƒíƒœ ì¶œë ¥ ---\n    print(f\"\\n=== í˜„ì¬ ì„¸ì…˜ ì™„ë£Œ ===\")\n    print(f\"ì™„ë£Œëœ ì—í­: {current_epoch-1}/{TARGET_TOTAL_EPOCHS}\")\n    print(f\"ìµœê³  ê²€ì¦ ì •í™•ë„: {best_val_acc:.4f}\")\n    \n    if current_epoch <= TARGET_TOTAL_EPOCHS:\n        print(f\"\\nâ° ë‹¤ìŒ ì„¸ì…˜ì—ì„œ ì—í­ {current_epoch}ë¶€í„° ì¬ê°œí•˜ì„¸ìš”!\")\n        print(\"ğŸ”§ ë‹¤ìŒ ì„¸ì…˜ ì‹¤í–‰ ì‹œ ì„¤ì •:\")\n        print(\"   USE_CHECKPOINT = True\")\n        print(f\"   CHECKPOINT_DIR = '{CHECKPOINT_DIR}'\")\n    else:\n        print(\"\\nğŸ‰ ëª¨ë“  í•™ìŠµì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!\")\n        \n        # ìµœì¢… í…ŒìŠ¤íŠ¸ í‰ê°€\n        print(\"\\n=== ìµœì¢… í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ í‰ê°€ ===\")\n        best_model = BertForSequenceClassification.from_pretrained('/kaggle/working/best_sentiment_model')\n        best_model.to(device)\n        \n        test_loss, test_acc, test_true, test_pred = evaluate_model(best_model, test_loader, device)\n        print(f\"í…ŒìŠ¤íŠ¸ ì†ì‹¤: {test_loss:.4f}\")\n        print(f\"í…ŒìŠ¤íŠ¸ ì •í™•ë„: {test_acc:.4f}\")\n        \n        # ìƒì„¸í•œ ë¶„ë¥˜ ì„±ëŠ¥ ë¦¬í¬íŠ¸\n        print(\"\\n=== ìƒì„¸ ë¶„ë¥˜ ì„±ëŠ¥ ë¦¬í¬íŠ¸ ===\")\n        target_names = ['ë§¤ìš° ë¶€ì •ì ', 'ë¶€ì •ì ', 'ê¸ì •ì ', 'ë§¤ìš° ê¸ì •ì ']\n        print(classification_report(test_true, test_pred, target_names=target_names))\n    \n    print(\"\\nâœ… í”„ë¡œê·¸ë¨ ì¢…ë£Œ\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import BertForSequenceClassification, BertTokenizer\nfrom sklearn.metrics import classification_report, accuracy_score\nfrom tqdm import tqdm\nimport os\n\n# --------------------------------------------------------------------------\n# 1. í‰ê°€ì— í•„ìš”í•œ í´ë˜ìŠ¤ì™€ í•¨ìˆ˜ ì •ì˜ (ê¸°ì¡´ ì½”ë“œì—ì„œ ê°€ì ¸ì˜´)\n# --------------------------------------------------------------------------\n\nclass ReviewDataset(Dataset):\n    \"\"\"PyTorchë¥¼ ìœ„í•œ ì»¤ìŠ¤í…€ ë°ì´í„°ì…‹ í´ë˜ìŠ¤\"\"\"\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        # ëª¨ë“  ì¸ì½”ë”© ê°’ì„ í…ì„œë¡œ ë³€í™˜\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        # ë ˆì´ë¸”ë„ í…ì„œë¡œ ë³€í™˜\n        item['labels'] = torch.tensor(self.labels[idx])\n        return item\n\n    def __len__(self):\n        return len(self.labels)\n\ndef evaluate_model(model, data_loader, device):\n    \"\"\"ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ê³  ì˜ˆì¸¡ ê²°ê³¼ì™€ ì‹¤ì œ ê°’ì„ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜\"\"\"\n    model.eval()  # ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •\n    \n    predictions = []\n    true_labels = []\n    total_loss = 0\n    \n    with torch.no_grad():  # ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° ë¹„í™œì„±í™”\n        for batch in tqdm(data_loader, desc=\"Evaluating\"):\n            # ë°ì´í„°ë¥¼ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['labels'].to(device)\n            \n            # ëª¨ë¸ ì‹¤í–‰\n            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n            \n            # ì†ì‹¤ê³¼ ì˜ˆì¸¡ ê²°ê³¼ ê³„ì‚°\n            loss = outputs.loss\n            total_loss += loss.item()\n            \n            preds = torch.argmax(outputs.logits, dim=-1)\n            \n            # ê²°ê³¼ ì €ì¥\n            predictions.extend(preds.cpu().numpy())\n            true_labels.extend(labels.cpu().numpy())\n            \n    avg_loss = total_loss / len(data_loader)\n    accuracy = accuracy_score(true_labels, predictions)\n    \n    return avg_loss, accuracy, true_labels, predictions\n\ndef load_data_split(save_path):\n    \"\"\"ì €ì¥ëœ ë°ì´í„° ë¶„í•  ê²°ê³¼ë¥¼ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜\"\"\"\n    try:\n        data_split = torch.load(save_path, map_location='cpu')\n        print(f\"âœ… '{save_path}'ì—ì„œ ë°ì´í„° ë¶„í•  ë¡œë“œ ì™„ë£Œ.\")\n        return (data_split['train_texts'], data_split['train_labels'],\n                data_split['val_texts'], data_split['val_labels'],\n                data_split['test_texts'], data_split['test_labels'])\n    except Exception as e:\n        print(f\"âŒ ë°ì´í„° ë¶„í•  ë¡œë“œ ì‹¤íŒ¨: {e}\")\n        return None\n\n# --------------------------------------------------------------------------\n# 2. ë©”ì¸ ì‹¤í–‰ë¶€\n# --------------------------------------------------------------------------\n\n# ğŸš¨ **ê²½ë¡œë¥¼ ìì‹ ì˜ í™˜ê²½ì— ë§ê²Œ ìˆ˜ì •í•˜ì„¸ìš”!** ğŸš¨\n# ê°€ì¥ ì„±ëŠ¥ì´ ì¢‹ì•˜ë˜ ëª¨ë¸ ê²½ë¡œ\nMODEL_PATH = '/kaggle/input/best_bert/pytorch/best_bert/1/best_sentiment_model'\n# í•™ìŠµ ì‹œ ë¶„í• í•´ë‘ì—ˆë˜ ë°ì´í„° ê²½ë¡œ\nDATA_SPLIT_PATH = '/kaggle/input/data-split/data_split.pth'\n\n# --- ì¥ì¹˜ ì„¤ì • ---\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}\")\n\n# --- ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ ---\nif not os.path.exists(MODEL_PATH):\n    print(f\"ì˜¤ë¥˜: '{MODEL_PATH}' ê²½ë¡œì— ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.\")\nelse:\n    print(f\"'{MODEL_PATH}'ì—ì„œ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤...\")\n    model = BertForSequenceClassification.from_pretrained(MODEL_PATH)\n    tokenizer = BertTokenizer.from_pretrained(MODEL_PATH)\n    model.to(device)\n    print(\"ëª¨ë¸ ë¡œë”© ì™„ë£Œ.\")\n\n    # --- ë°ì´í„° ë¡œë“œ ë° ì¤€ë¹„ ---\n    if not os.path.exists(DATA_SPLIT_PATH):\n        print(f\"ì˜¤ë¥˜: '{DATA_SPLIT_PATH}' ê²½ë¡œì— ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.\")\n    else:\n        # ì´ì „ì— ë¶„í• í•´ ë‘” ë°ì´í„° ë¡œë“œ (í•™ìŠµ/ê²€ì¦ ë°ì´í„°ëŠ” í•„ìš” ì—†ì§€ë§Œ test ë°ì´í„°ë¥¼ ìœ„í•´ ë¡œë“œ)\n        _, _, _, _, test_texts, test_labels = load_data_split(DATA_SPLIT_PATH)\n\n        # í…ŒìŠ¤íŠ¸ ë°ì´í„° í† í°í™”\n        print(\"í…ŒìŠ¤íŠ¸ ë°ì´í„° í† í°í™” ì¤‘...\")\n        test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=128)\n\n        # ë°ì´í„°ì…‹ ë° ë°ì´í„°ë¡œë” ìƒì„±\n        test_dataset = ReviewDataset(test_encodings, test_labels)\n        test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n        print(\"ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ.\")\n\n        # --- ëª¨ë¸ í‰ê°€ ì‹¤í–‰ ---\n        print(\"\\nğŸš€ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.\")\n        test_loss, test_acc, test_true, test_pred = evaluate_model(model, test_loader, device)\n\n        # --- ìµœì¢… ê²°ê³¼ ì¶œë ¥ ---\n        print(\"\\nğŸ“Š ìµœì¢… í‰ê°€ ê²°ê³¼\")\n        print(\"=\"*30)\n        print(f\"í…ŒìŠ¤íŠ¸ ì†ì‹¤ (Test Loss): {test_loss:.4f}\")\n        print(f\"í…ŒìŠ¤íŠ¸ ì •í™•ë„ (Test Accuracy): {test_acc:.4f}\")\n        print(\"=\"*30)\n\n        print(\"\\nğŸ“‹ ìƒì„¸ ë¶„ë¥˜ ì„±ëŠ¥ ë¦¬í¬íŠ¸ (Classification Report)\")\n        target_names = ['ë§¤ìš° ë¶€ì •ì  (0)', 'ë¶€ì •ì  (1)', 'ê¸ì •ì  (2)', 'ë§¤ìš° ê¸ì •ì  (3)']\n        print(classification_report(test_true, test_pred, target_names=target_names, digits=4))","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}